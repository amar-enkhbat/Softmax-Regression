{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing basic Softmax Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello, everyone. It's me again. This time I would like to introduce Softmax Regression or Multinomial Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing's first. Softmax regression is NOT a regression model. It's a like Logistic Regression probabilistic classifier. For some historic reason the term regression stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "  - One-vs-Rest Logistic Regression\n",
    "  - Softmax Regression\n",
    "  - Softmax regression intuition\n",
    "  - The Softmax function\n",
    "  - The Cross-Entropy loss function\n",
    "  - Training\n",
    "  - Validation\n",
    "  - Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-vs-all Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-vs-all logistic regression for multiclass problems is simple. You simply train $n$ differet classifiers for $n$ different classes, where the intended class is 1 and others 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a pseudocode the algorithm is as follows: <br>\n",
    "Inputs: \n",
    "  - $L$, a learner\n",
    "  - Samples $X$\n",
    "  - Labels $y$ where $y \\in \\{1, ..., K \\}$ <br>\n",
    "Output:\n",
    "  - a list of classifiers $f_k$ for $k \\in \\{1, ..., K \\}$ <br>\n",
    "Procedure:\n",
    "  - For each $k$ in $\\{1,...,K\\}$\n",
    "    - Construct a new label vector $z$ where $z_i=1$ if $y_i=k$ and $z_i = 0$ otherwise\n",
    "    - Apply $L$ to $X$, $z$ to obtain $f_k$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making decisions means applying all classifier to an unseen sample $x$ and predicting label $k$ for which the corresponding classifier reports the highest confidence score:\n",
    "$$\\widehat{y}=argmax_{k \\in \\{1,...,K\\}} f_k(x)$$\n",
    "I copied the above pseudocode shamelessly from wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a better understanding let's run a OvR (One-vs-rest) Logistic regression and see its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# This time we're going to use a much better dataset than previous simple datapoints.\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "import os\n",
    "import struct\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 3]]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's plot our data samples to see what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  [0 1 2]\n",
      "Number of samples: 150\n",
      "Number of features: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"Labels: \", np.unique(y))\n",
    "print(\"Number of samples:\", X.shape[0])\n",
    "print(\"Number of features:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our data has 150 samples in 2 features in 3 different classes ($k = \\{0, 1, 2\\}$). It only has two features because it's easier to plot and visualize. <br>\n",
    "Let's train a OvR classifier on the data and get some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix:\n",
      "[[50  0  0]\n",
      " [ 0 38 12]\n",
      " [ 0  2 48]]\n",
      "Accuracy:\n",
      "0.9066666666666666\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter = 100)\n",
    "multi_classifier = OneVsRestClassifier(classifier)\n",
    "multi_classifier.fit(X, y)\n",
    "y_pred = multi_classifier.predict(X)\n",
    "print(\"The confusion matrix:\")\n",
    "print(confusion_matrix(y, y_pred))\n",
    "print(\"Accuracy:\")\n",
    "print(accuracy_score(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With OvR Logistic Regression we get accuracy of 90.6% and the decision boundary looks good. All is well, right?\n",
    "Let's see the probabilities for the first 3 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "y_proba = multi_classifier.predict_proba(X)\n",
    "print(y_proba[:10].sum(axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
